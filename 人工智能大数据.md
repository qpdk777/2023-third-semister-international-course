# 概论

机器学习是人工智能的分支，使用训练数据提升表现的算法。典型的机器学习包含很多参数，这些参数从数据中学习而来。

$深度学习 \subsetneqq 机器学习 \subsetneqq 人工智能$

传统编程：$数据+程序\xrightarrow{电脑}输出$

机器学习：$数据+输出\xrightarrow{电脑}程序$

现代AI发展的条件：数据量大，算力充足

分类：监督学习，无监督学习，半监督学习，强化学习

* 监督学习：神经网络、支持向量机、深度信念网络

* 无监督学习：K均值、基于图的方法

* 计算机视觉应用


# 监督学习（分类）

分类：离散型问题

回归：连续型问题

根据 $(X_1,Y_1),(X_2,Y_2)...(X_n,Y_n)$ 求出分类器 $f:X\rightarrow Y$ 。

假设：
* 在 $X\times Y$ 上存在联合概率分布 $P$ 。
* 训练样本 $(X_i,Y_i)$ 独立地从 $P$ 中抽样。
  
损失函数：用于衡量分类的“费用”。

0-1损失函数：

$$
\begin{equation}
    l(X,Y,f(X))=\left\{
        \begin{aligned}
        1, &&&{\rm if} f(X)\ne Y \\
        0, &&&{\rm otherwise}
        \end{aligned}
        \right.
\end{equation}
$$

风险函数是损失函数在 $P$ 分布下的均值。

$$R(f)=E(X,Y,f(X))=\sum_X \sum_Y P(X,Y)l(X,Y,f(X))$$

所有的分类器中，贝叶斯分类器是最好的

$$
\begin{equation}
f_{Bayes}(x)=
    \left\{
        \begin{aligned}
        1, &&& {\rm if} P(Y=1|X=x)\ge0.5 \\
        -1, &&& {\rm otherwise}
        \end{aligned}
    \right .
\end{equation}
$$

最近邻规则（Nearest Neighbor Rule）

$$R(f_{Bayes})\le R_\infty\le 2R(f_{Bayes})$$

$R_\infty$ 表示当样本大小趋于无穷大时NN的预期误差率。

k-NN rule: 用最邻近的k个邻居作为一个主要的依据

$\rm{k_n}$-NN rule: 用最邻近的$k_n$个邻居作为一个主要的依据，k随n而变化。

如果$n\rightarrow\infty$且$k\rightarrow\infty$，则$k/n\rightarrow0$，对于所有概率分布$P$，都有$R(k_n\text{-NN})\rightarrow R(f_{Bayes})$，达到普遍贝叶斯一致。

# 无监督学习（聚类分析）

给定一组$n$个物体和一个描述两两之间相似性的$n\times n$矩阵 $\mathbf{A}$，本质是一个有边权的图$G$。

目标是将图$G$划分成若干最大的同类集群。

聚类分析等无监督学习无须提供大量样本，比监督学习成本更低。

K均值

* 初始化：随机选择k个点作为聚类中心
* 变换：
  * 1. 将数据点分配到最近的聚类中心
  * 2. 将聚类中心改变到其分配到的点的平均值处
* 停止：当没有点的分配被改变时

无穷多步后必然达到收敛

该算法最小化了目标函数

$$\sum_{i\in \text{clusters}}\left\{\sum_{j\in \text{elements of i'th cluster}} ||x_j-\mu_i||^2\right\}$$

其中$\mu_i$是第$i$个聚类的中心。

优点：非常简单、高效

缺点：
* 只能收敛到误差函数的局部最小值
* 需要合理地选择k值
* 对初始化太敏感
* 对异常值太敏感
* 只能找到“球状”的聚类
# 概论

机器学习是人工智能的分支，使用训练数据提升表现的算法。典型的机器学习包含很多参数，这些参数从数据中学习而来。

$深度学习 \subsetneqq 机器学习 \subsetneqq 人工智能$

传统编程：$数据+程序\xrightarrow{电脑}输出$

机器学习：$数据+输出\xrightarrow{电脑}程序$

现代AI发展的条件：数据量大，算力充足

分类：监督学习，无监督学习，半监督学习，强化学习

* 监督学习：神经网络、支持向量机、深度信念网络

* 无监督学习：K均值、基于图的方法

* 计算机视觉应用


# 监督学习（分类）

分类：离散型问题

回归：连续型问题

根据 $(X_1,Y_1),(X_2,Y_2)...(X_n,Y_n)$ 求出分类器 $f:X\rightarrow Y$ 。

假设：
* 在 $X\times Y$ 上存在联合概率分布 $P$ 。
* 训练样本 $(X_i,Y_i)$ 独立地从 $P$ 中抽样。
  
损失函数：用于衡量分类的“费用”。

0-1损失函数：

$$
\begin{equation}
    l(X,Y,f(X))=\left\{
        \begin{aligned}
        1, &&&{\rm if} \ f(X)\ne Y \\
        0, &&&{\rm otherwise}
        \end{aligned}
        \right.
\end{equation}
$$

风险函数是损失函数在 $P$ 分布下的均值。

$$R(f)=E(X,Y,f(X))=\sum_X \sum_Y P(X,Y)l(X,Y,f(X))$$

所有的分类器中，贝叶斯分类器是最好的

$$
\begin{equation}
f_{Bayes}(x)=
    \left\{
        \begin{aligned}
        1, &&& {\rm if} \ P(Y=1|X=x)\ge0.5 \\
        -1, &&& {\rm otherwise}
        \end{aligned}
    \right .
\end{equation}
$$

最近邻规则（Nearest Neighbor Rule）

$$R(f_{Bayes})\le R_\infty\le 2R(f_{Bayes})$$

$R_\infty$ 表示当样本大小趋于无穷大时NN的预期误差率。

k-NN rule: 用最邻近的k个邻居作为一个主要的依据

$\rm{k_n}$-NN rule: 用最邻近的$k_n$个邻居作为一个主要的依据，k随n而变化。

如果$n\rightarrow\infty$且$k\rightarrow\infty$，则$k/n\rightarrow0$，对于所有概率分布$P$，都有$R(k_n\text{-NN})\rightarrow R(f_{Bayes})$，达到普遍贝叶斯一致。

# 无监督学习（聚类分析）

给定一组$n$个物体和一个描述两两之间相似性的$n\times n$矩阵 $\mathbf{A}$，本质是一个有边权的图$G$。

目标是将图$G$划分成若干最大的同类集群。

聚类分析等无监督学习无须提供大量样本，比监督学习成本更低。

K均值

* 初始化：随机选择k个点作为聚类中心
* 变换：
  * 1. 将数据点分配到最近的聚类中心
  * 2. 将聚类中心改变到其分配到的点的平均值处
* 停止：当没有点的分配被改变时

无穷多步后必然达到收敛

该算法最小化了目标函数

$$\sum_{i\in \text{clusters}}\left\{\sum_{j\in \text{elements of i'th cluster}} ||x_j-\mu_i||^2\right\}$$

其中$\mu_i$是第$i$个聚类的中心。

优点：非常简单、高效

缺点：
* 只能收敛到误差函数的局部最小值
* 需要合理地选择k值
* 对初始化太敏感
* 对异常值太敏感
* 只能找到“球状”的聚类

# 神经网络

神经元结构：

* 细胞体（cell body）：计算
* 树突（dendrite）：输入，有很多条
* 轴突（axon）：输出，只有一条，但末端分叉出很多突触（synapse），可以与多个神经元连接

工作机制：

$$电信号\rightarrow化学信号\rightarrow电信号$$

两种状态：

* 兴奋（Excitatory）
* 抑制（Inhibitory）

------------------------

McCulloch-Pitts(MP)神经元是一种二元临界单元模型

若干输入$l_1,l_2,...,l_n$被分配若干权重$w_1,w_2,...,w_n$，输出$y$为。

$$y=g\left(\sum_j w_jl_j-T \right)$$

其中$T$代表临界值。

其中$g$的解析式为s

$$
\begin{equation}
    g(x)=\left\{
        \begin{aligned}
            0, &&& {\rm if} \ x<0 \\
            1, &&& {\rm if} \ x\ge0
        \end{aligned}
        \right.
\end{equation}
$$

---------------------------

逻辑门

* 非门：输入为假时，输出真
* 与门：输入均为真时，输出真
* 或门：输入有一个为真是，输出真

---------------------
---------------------

参数太少：欠拟合

参数太多：过拟合

# 支持向量机

支持向量：

$${\rm w}^T+b=1$$

$${\rm w}^T+b=0$$

$${\rm w}^T+b=-1$$

间隔: $\frac{2}{||{\rm w}^T||}$

给定训练集$L={({\rm x}_1,y_1),({\rm x}_2,y_2),...,({\rm x}_n,y_n)}$，学习支持向量机可以被形式化为一个最优化问题：

$$\max_{\rm w}\frac{2}{||{\rm w}||} \ {\rm s.t.} \ {\rm w}^T{\rm x}_i+b \ 
\begin{aligned}
    \ge 1 && {\rm if} \ y_i=+1\\
    \le 1 && {\rm if} \ y_i=-1
\end{aligned}
\ {\rm for} \ i=1,2,...,n
$$

异常值处理方法：软间隔

$$\min\frac{1}{2}||{\rm w}||^2+C\sum_{i=1}^N \xi_i \\ {\rm s.t.}
\begin{aligned}
    &&& y_i({\rm w}^T+b)\ge1-\xi_i \\
    &&& \xi_i\ge0
\end{aligned}
\ {\rm for} \ i=1,2,...,n

 $$